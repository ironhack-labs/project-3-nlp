{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An치lisis Exploratorio de Datos - Clasificaci칩n de Traducciones,\n",
    "\n",
    "    Este notebook contiene el an치lisis exploratorio de los datos de traducciones humanas vs. autom치ticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models Results\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This document presents the results of different machine learning models applied to the translation classification task, with the objective of distinguishing between human and machine translations in Spanish texts.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "- Total samples: ~18,000 texts\n",
    "- Class distribution: Balanced (50% human, 50% machine)\n",
    "- Text characteristics:\n",
    " - Extracted features: character length, word count, average word length, punctuation, uppercase letters, numbers\n",
    " - Language: Spanish\n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "#NOTE: These pre-processing was employed on all the models excepts the Transformer Models. For the Tranformer model we only clean the extra spaces and convert upper case latters to lower cases.\n",
    "\n",
    "- Text cleaning: \n",
    " - Space and punctuation normalization\n",
    " - Special characters removal\n",
    " - Lowercase conversion\n",
    " - Lemmatization using spaCy (es_core_news_md model)\n",
    " - Stopwords removal using NLTK\n",
    "\n",
    "\n",
    "- Feature extraction:\n",
    " - Text length (characters)\n",
    " - Word count\n",
    " - Average word length\n",
    " - Punctuation count\n",
    " - Uppercase letters count\n",
    " - Numbers count\n",
    "\n",
    "\n",
    "- Vectorization:\n",
    " - TF-IDF with max_features=5000\n",
    " - Parameters:\n",
    "   - min_df=2\n",
    "   - max_df=0.95\n",
    "   - ngram_range=(1,1) for unigrams\n",
    "   - ngram_range=(2,2) for bigrams\n",
    "   - ngram_range=(3,3) for trigrams\n",
    "\n",
    "\n",
    "- Normalization:\n",
    "Note: For clusters Method\n",
    " - StandardScaler for numerical features\n",
    " - Text normalization using spaCy\n",
    "\n",
    "\n",
    "- Data split:\n",
    " - Standard train/test split\n",
    " - Maintaining class balance\n",
    "\n",
    "\n",
    "## Supervised Models\n",
    "\n",
    "\n",
    "### Classical Models\n",
    "\n",
    "\n",
    "#### Naive Bayes\n",
    "- Model: MultinomialNB()\n",
    "- Vectorization: TF-IDF (max_features=5000)\n",
    "- Accuracy: 0.43\n",
    "- Confusion Matrix: [[798, 990], [1056, 732]]\n",
    "- Precision: 0.42\n",
    "- Recall: 0.42\n",
    "- F1-score: 0.42\n",
    "\n",
    "\n",
    "#### Logistic Regression\n",
    "- Model: LogisticRegression(max_iter=1000)\n",
    "- Vectorization: TF-IDF (max_features=5000)\n",
    "- Accuracy: 0.46\n",
    "- Confusion Matrix: [[841, 947], [983, 805]]\n",
    "- Precision: 0.46\n",
    "- Recall: 0.46\n",
    "- F1-score: 0.46\n",
    "\n",
    "\n",
    "#### Linear SVM\n",
    "- Model: LinearSVC(max_iter=1000)\n",
    "- Vectorization: TF-IDF (max_features=5000)\n",
    "- Accuracy: 0.44\n",
    "- Confusion Matrix: [[793, 995], [1011, 777]]\n",
    "- Precision: 0.44\n",
    "- Recall: 0.44\n",
    "- F1-score: 0.44\n",
    "\n",
    "\n",
    "### Transformer Models\n",
    "\n",
    "\n",
    "#### BETO\n",
    "- Model: dccuchile/bert-base-spanish-wwm-uncased\n",
    "- Parameters:\n",
    " - Batch size: 32\n",
    " - Epochs: 5\n",
    " - Learning rate: 2e-5\n",
    " - Optimizer: AdamW\n",
    " - Max sequence length: 512\n",
    "- Accuracy: 0.592\n",
    "- Final average loss: 0.203\n",
    "- Confusion Matrix: [[981,770],[678,1147]] \n",
    "\n",
    "\n",
    "#### RoBERTa-Spanish\n",
    "- Model: BSC-TeMU/roberta-base-bne\n",
    "- Parameters:\n",
    " - Batch size: 32\n",
    " - Epochs: 5\n",
    " - Learning rate: 2e-5\n",
    " - Optimizer: AdamW\n",
    " - Max sequence length: 512\n",
    "- Accuracy: 0.62\n",
    "- Final average loss: 0.13\n",
    "- Confusion Matrix: [[1158,593],[767,1058]] \n",
    "\n",
    "\n",
    "### N-grams Analysis\n",
    "\n",
    "\n",
    "#### Unigrams\n",
    "- Best features: TF-IDF with max_features=5000\n",
    "- Parameters:\n",
    " - min_df=2\n",
    " - max_df=0.95\n",
    " - ngram_range=(1,1)\n",
    "- Accuracy: 0.42\n",
    "\n",
    "\n",
    "#### Bigrams\n",
    "- Best features: TF-IDF with max_features=5000\n",
    "- Parameters:\n",
    " - min_df=2\n",
    " - max_df=0.95\n",
    " - ngram_range=(2,2)\n",
    "- Accuracy: 0.44\n",
    "\n",
    "\n",
    "#### Trigrams\n",
    "- Best features: TF-IDF with max_features=5000\n",
    "- Parameters:\n",
    " - min_df=2\n",
    " - max_df=0.95\n",
    " - ngram_range=(3,3)\n",
    "- Accuracy: 0.41\n",
    "\n",
    "\n",
    "## Unsupervised Models\n",
    "\n",
    "\n",
    "### K-means Clustering\n",
    "\n",
    "\n",
    "#### Parameters:\n",
    "- n_clusters: 2\n",
    "- random_state: 42\n",
    "- n_init: 10 (default)\n",
    "- Normalization: StandardScaler()\n",
    "- Visualization: PCA (2 components)\n",
    "\n",
    "\n",
    "#### Results by feature set:\n",
    "- Basic (char_length, word_count):\n",
    " - Silhouette Score: 0.593\n",
    " - PCA Variance Explained: 1.000\n",
    "\n",
    "\n",
    "- Length (char_length, word_count, avg_word_length):\n",
    " - Silhouette Score: 0.413\n",
    " - PCA Variance Explained: 0.998\n",
    "\n",
    "\n",
    "- Punctuation (puntuacion, mayusculas):\n",
    " - Silhouette Score: 0.557\n",
    " - PCA Variance Explained: 1.000\n",
    "\n",
    "\n",
    "- Complete (all features):\n",
    " - Silhouette Score: 0.357\n",
    " - PCA Variance Explained: 0.626\n",
    "\n",
    "\n",
    "### DBSCAN Clustering\n",
    "\n",
    "\n",
    "#### Tested Parameters:\n",
    "- eps: [0.3, 0.5, 0.7]\n",
    "- min_samples: [5, 10, 15]\n",
    "- Normalization: StandardScaler()\n",
    "- Visualization: PCA (2 components)\n",
    "\n",
    "\n",
    "#### Best Configuration:\n",
    "- eps: 0.7\n",
    "- min_samples: 10\n",
    "- Silhouette Score: 0.170\n",
    "- Comparison with K-means: 0.232\n",
    "- Number of identified clusters: 4 + noise\n",
    "- Cluster characteristics:\n",
    " - Cluster 0: Average texts (16,228 samples)\n",
    " - Cluster 1: High punctuation texts (12 samples)\n",
    " - Cluster 2: Short texts with uppercase and numbers (10 samples)\n",
    " - Cluster 3: Very short texts with numbers (10 samples)\n",
    " - Noise: Atypical texts (1,617 samples)\n",
    "\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "\n",
    "1. Transformer models (BETO and RoBERTa-Spanish) achieved the best results with accuracies of 0.592 and 0.62 respectively.\n",
    "\n",
    "\n",
    "2. Among classical models, Logistic Regression performed best with an accuracy of 0.46.\n",
    "\n",
    "\n",
    "3. N-grams analysis showed that bigrams were more effective (0.44) than unigrams (0.42) and trigrams (0.41).\n",
    "\n",
    "\n",
    "4. In unsupervised clustering:\n",
    "  - K-means worked better with basic features (character length and word count)\n",
    "  - DBSCAN identified interesting groups but with less cohesion than K-means\n",
    "  - Most texts were grouped in one main cluster, with small clusters for special cases\n",
    "\n",
    "\n",
    "5. Supervised classification was more effective than unsupervised clustering for distinguishing between human and machine translations.\n",
    "\n",
    "\n",
    "## Evaluation Metrics\n",
    "### Description of metrics used:\n",
    "- Accuracy: Proportion of correct predictions\n",
    "- Precision: Proportion of correct positive predictions\n",
    "- Recall: Proportion of identified positive cases\n",
    "- F1-score: Harmonic mean between precision and recall\n",
    "- Silhouette Score: Measure of cluster cohesion (-1 to 1)\n",
    "- Adjusted Rand Score: Similarity between clusters and real labels (-1 to 1)\n",
    "\n",
    "\n",
    "## Limitations and Future Work\n",
    "1. Identified limitations:\n",
    "  - Difficulty in detecting high-quality translations\n",
    "  - Extensive training time for transformer models\n",
    "  - Need for significant computational resources\n",
    "\n",
    "\n",
    "2. Possible improvements:\n",
    "  - Experiment with other pre-trained models\n",
    "  - Increase dataset with more examples\n",
    "  - Incorporate additional linguistic features\n",
    "  - Test ensemble learning techniques\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
