{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9c1115-8dab-4e27-962c-157cf51bbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "#!pip install tensorflow-macos tensorflow-metal\n",
    "\n",
    "\n",
    "# Set MAC to use Sillicon (MPS - Metal Performance Shaders), GPU/CPU\n",
    "\n",
    "# Chek with Pytorch\n",
    "# import torch\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"MPS available.\")\n",
    "# else:\n",
    "#     print(\"MPS not available.\")\n",
    "    \n",
    "# device = torch.device(\"mps\")\n",
    "\n",
    "# # Check with Tensorflow\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Check if GPU is used\n",
    "# print(\"It's using the GPU?:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba07f3-02c2-4e58-a8d2-2c353876b3af",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition and Loading\n",
    "\n",
    "- Objective: Load the dataset containing the sentences and corresponding labels (0 for machine translation, 1 for human translation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd79549-1386-4276-935d-9c31a99f8c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17877 entries, 0 to 17876\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   17877 non-null  int64 \n",
      " 1   Text    17877 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 279.5+ KB\n",
      "None\n",
      "\n",
      "Training Data Sample:\n",
      "   Label                                               Text\n",
      "0      1  Cuando conocí a Janice en 2013 , una familia n...\n",
      "1      0  Hwang habló en Sur de este año por Southwest M...\n",
      "2      1  Usted podría pensar Katy Perry y Robert Pattin...\n",
      "3      1  Cualquiera que haya volado los cielos del crea...\n",
      "4      1  Bueno , este cantante tendrá un LARGO tiempo p...\n",
      "\n",
      "Real Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2201 entries, 2 to 2\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    2201 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 34.4+ KB\n",
      "None\n",
      "\n",
      "Real Data Sample:\n",
      "                                                Text\n",
      "2  Yo no creo que a nadie le haya encantado un pe...\n",
      "2  No va a resolver sus problemas de crédito o me...\n",
      "2                                Te encantará este !\n",
      "2  Yo estaba a volar a un aeropuerto varias horas...\n",
      "2  ( Maid En Manhattan , The Wedding Planner , Je...\n",
      "\n",
      "Missing values in training data: Label    0\n",
      "Text     0\n",
      "dtype: int64\n",
      "Missing values in real data: Text    0\n",
      "dtype: int64\n",
      "\n",
      "Label Distribution in Training Data:\n",
      "Label\n",
      "0    8939\n",
      "1    8938\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "# Load the training and validation datasets with error handling\n",
    "training_data = pd.read_csv(\n",
    "    'TRAINING_DATA.txt',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=[\"Label\", \"Text\"],\n",
    "    quoting=3,  # To ignore special quoting issues\n",
    ")\n",
    "\n",
    "real_data = pd.read_csv(\n",
    "    'REAL_DATA.txt',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=[\"Text\"],\n",
    "    quoting=3,\n",
    ")\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Training Dataset Info:\")\n",
    "print(training_data.info())\n",
    "print(\"\\nTraining Data Sample:\")\n",
    "print(training_data.head())\n",
    "\n",
    "print(\"\\nReal Dataset Info:\")\n",
    "print(real_data.info())\n",
    "print(\"\\nReal Data Sample:\")\n",
    "print(real_data.head())\n",
    "\n",
    "# Check for missing values in both datasets\n",
    "print(\"\\nMissing values in training data:\", training_data.isnull().sum())\n",
    "print(\"Missing values in real data:\", real_data.isnull().sum())\n",
    "\n",
    "print(\"\\nLabel Distribution in Training Data:\")\n",
    "print(training_data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5b6a4-153d-48b7-9be6-f34e112869a7",
   "metadata": {},
   "source": [
    "## 2. Text Extraction and Cleanup\n",
    "\n",
    "- Objective: Ensure that sentences are properly extracted and cleaned before further processing.\n",
    "\n",
    "This step involves cleaning the text data to ensure that the sentences are free from noise and unnecessary characters. The main tasks will include:\n",
    "- Removing special characters, URLs, and digits.\n",
    "- Converting the text to lowercase.\n",
    "- Removing extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9824715c-6a12-43e2-ad1d-cd8084865471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dastas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Cleaned Training Data Sample:\n",
      "                                                Text  \\\n",
      "0  Cuando conocí a Janice en 2013 , una familia n...   \n",
      "1  Hwang habló en Sur de este año por Southwest M...   \n",
      "2  Usted podría pensar Katy Perry y Robert Pattin...   \n",
      "3  Cualquiera que haya volado los cielos del crea...   \n",
      "4  Bueno , este cantante tendrá un LARGO tiempo p...   \n",
      "\n",
      "                                        Cleaned_Text  \\\n",
      "0  cuando conocí a janice en una familia necesita...   \n",
      "1  hwang habló en sur de este año por southwest m...   \n",
      "2  usted podría pensar katy perry y robert pattin...   \n",
      "3  cualquiera que haya volado los cielos del crea...   \n",
      "4  bueno este cantante tendrá un largo tiempo par...   \n",
      "\n",
      "                                  Final_Cleaned_Text  \n",
      "0  conocí janice familia necesitar punto promedio...  \n",
      "1  hwang hablar sur año southwest music and media...  \n",
      "2  usted poder pensar katy perry robert pattinson...  \n",
      "3  cualquiera volar cielo creador escuchar acto p...  \n",
      "4  bueno cantante largo tiempo sentir aún remordi...  \n",
      "\n",
      "Enhanced Cleaned Real Data Sample:\n",
      "                                                Text  \\\n",
      "2  Yo no creo que a nadie le haya encantado un pe...   \n",
      "2  No va a resolver sus problemas de crédito o me...   \n",
      "2                                Te encantará este !   \n",
      "2  Yo estaba a volar a un aeropuerto varias horas...   \n",
      "2  ( Maid En Manhattan , The Wedding Planner , Je...   \n",
      "\n",
      "                                        Cleaned_Text  \\\n",
      "2  yo no creo que a nadie le haya encantado un pe...   \n",
      "2  no va a resolver sus problemas de crédito o me...   \n",
      "2                                  te encantará este   \n",
      "2  yo estaba a volar a un aeropuerto varias horas...   \n",
      "2  maid en manhattan the wedding planner jersey g...   \n",
      "\n",
      "                                  Final_Cleaned_Text  \n",
      "2                  creer nadie encantar pene flácido  \n",
      "2  ir resolver problema crédito mejorar relación ...  \n",
      "2                                           encantar  \n",
      "2  volar aeropuerto varios hora distancia alquila...  \n",
      "2  maid manhattan the wedding planner jersey girl...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download Spanish stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy's Spanish model for lemmatization\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Set of Spanish stopwords\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by performing the following:\n",
    "    - Lowercasing the text.\n",
    "    - Removing URLs.\n",
    "    - Removing special characters, digits, and punctuation.\n",
    "    - Stripping extra whitespaces.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters, digits, and punctuation\n",
    "    text = re.sub(r'[^a-záéíóúñü\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords and applies lemmatization using spaCy (better than nltk).\n",
    "    Tokenizes the final cleaned and processed text.\n",
    "    \"\"\"\n",
    "    # Tokenize and lemmatize using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Remove stopwords and perform lemmatization\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_punct]\n",
    "    \n",
    "    # Join tokens into a cleaned sentence\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the cleaning function\n",
    "training_data['Cleaned_Text'] = training_data['Text'].apply(clean_text)\n",
    "real_data['Cleaned_Text'] = real_data['Text'].apply(clean_text)\n",
    "\n",
    "# Apply stopword removal and lemmatization\n",
    "training_data['Final_Cleaned_Text'] = training_data['Cleaned_Text'].apply(remove_stopwords_and_lemmatize)\n",
    "real_data['Final_Cleaned_Text'] = real_data['Cleaned_Text'].apply(remove_stopwords_and_lemmatize)\n",
    "\n",
    "# Display some cleaned samples\n",
    "print(\"\\nEnhanced Cleaned Training Data Sample:\")\n",
    "print(training_data[['Text', 'Cleaned_Text', 'Final_Cleaned_Text']].head())\n",
    "\n",
    "print(\"\\nEnhanced Cleaned Real Data Sample:\")\n",
    "print(real_data[['Text', 'Cleaned_Text', 'Final_Cleaned_Text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4836e3-2041-4fa0-8a6f-24f937297b27",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "Objective: Transform the raw text into a format that can be fed into a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d8ea1c-090b-4850-b55f-bb0ff2894687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix for Training Data:\n",
      "Shape: (17877, 5000)\n",
      "\n",
      "TF-IDF Matrix for Real Data:\n",
      "Shape: (2201, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer with n-grams (unigrams + bigrams)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit the number of features to speed up computation\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    stop_words=None,  # We already removed stopwords during preprocessing\n",
    "    sublinear_tf=True  # Apply sublinear TF scaling (logarithmic transformation)\n",
    ")\n",
    "\n",
    "# Fit the TF-IDF vectorizer to the cleaned training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(training_data['Final_Cleaned_Text'])\n",
    "X_real_tfidf = tfidf_vectorizer.transform(real_data['Final_Cleaned_Text'])\n",
    "\n",
    "# Display the shape of the resulting matrices\n",
    "print(\"\\nTF-IDF Matrix for Training Data:\")\n",
    "print(f\"Shape: {X_train_tfidf.shape}\")  # Should be (num_samples, num_features)\n",
    "\n",
    "print(\"\\nTF-IDF Matrix for Real Data:\")\n",
    "print(f\"Shape: {X_real_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0db26-2a33-4e83-9496-24da7454c64b",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Objective: Extract useful features that enhance model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1939cd7c-6ba1-46c6-8165-79df2391a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Training Features Shape: (17877, 5004)\n",
      "Combined Real Data Features Shape: (2201, 5004)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def extract_additional_features(texts):\n",
    "    \"\"\"\n",
    "    Extracts additional features to enhance model performance:\n",
    "    - Sentence length (in characters and words)\n",
    "    - Average word length\n",
    "    - Punctuation density\n",
    "    \"\"\"\n",
    "    # Calculate sentence length in characters\n",
    "    char_lengths = np.array([len(text) for text in texts]).reshape(-1, 1)\n",
    "\n",
    "    # Calculate sentence length in words\n",
    "    word_lengths = np.array([len(text.split()) for text in texts]).reshape(-1, 1)\n",
    "\n",
    "    # Calculate average word length\n",
    "    avg_word_lengths = np.array([np.mean([len(word) for word in text.split()]) if len(text.split()) > 0 else 0\n",
    "                                 for text in texts]).reshape(-1, 1)\n",
    "\n",
    "    # Calculate punctuation density (percentage of punctuation characters)\n",
    "    punctuation_counts = np.array([sum(1 for char in text if char in \".,!?;:\") / len(text)\n",
    "                                   if len(text) > 0 else 0 for text in texts]).reshape(-1, 1)\n",
    "\n",
    "    return np.hstack([char_lengths, word_lengths, avg_word_lengths, punctuation_counts])\n",
    "\n",
    "# Extract features for training and real datasets\n",
    "additional_features_train = extract_additional_features(training_data['Final_Cleaned_Text'])\n",
    "additional_features_real = extract_additional_features(real_data['Final_Cleaned_Text'])\n",
    "\n",
    "# Combine TF-IDF features with additional features\n",
    "X_train_combined = hstack([X_train_tfidf, additional_features_train])\n",
    "X_real_combined = hstack([X_real_tfidf, additional_features_real])\n",
    "\n",
    "print(f\"\\nCombined Training Features Shape: {X_train_combined.shape}\")\n",
    "print(f\"Combined Real Data Features Shape: {X_real_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d227d8-887a-4ecf-9b3b-e7c99e7cf06b",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Training\n",
    "\n",
    "Objective: Train a classifier to distinguish between machine and human translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9971f28-f109-41f6-b664-d5632eedfd57",
   "metadata": {},
   "source": [
    "### 5.1 Model Selection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c37643-ec9c-44e4-8d76-cfe67bca98e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models using cross-validation on combined features...\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression: Mean Accuracy = 0.4090, Std = 0.0086\n",
      "\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest: Mean Accuracy = 0.2700, Std = 0.0074\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "Logistic Regression: Mean Accuracy = 0.4090, Std = 0.0086\n",
      "Random Forest: Mean Accuracy = 0.2700, Std = 0.0074\n",
      "\n",
      "Selected Best Model: Logistic Regression (Mean Accuracy = 0.4090)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Defined  y_train; it's not necessary if we use training_data['Label'].values\n",
    "y_train = training_data['Label'].values  # Get the target labels\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='liblinear'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Store evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Perform cross-validation for each model using the combined feature set\n",
    "print(\"Evaluating models using cross-validation on combined features...\\n\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    scores = cross_val_score(model, X_train_combined, y_train, cv=5, scoring='accuracy')\n",
    "    evaluation_results[model_name] = {\n",
    "        \"Mean Accuracy\": np.mean(scores),\n",
    "        \"Std Accuracy\": np.std(scores)\n",
    "    }\n",
    "    print(f\"{model_name}: Mean Accuracy = {np.mean(scores):.4f}, Std = {np.std(scores):.4f}\\n\")\n",
    "\n",
    "# Display all model performance results\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "for model_name, result in evaluation_results.items():\n",
    "    print(f\"{model_name}: Mean Accuracy = {result['Mean Accuracy']:.4f}, Std = {result['Std Accuracy']:.4f}\")\n",
    "\n",
    "# Automatically select the best model\n",
    "best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k]['Mean Accuracy'])\n",
    "print(f\"\\nSelected Best Model: {best_model_name} (Mean Accuracy = {evaluation_results[best_model_name]['Mean Accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6dd22-7352-43cf-85bd-47ad6e422914",
   "metadata": {},
   "source": [
    "#### Logistic Regression outperforms Random Forest based on mean accuracy, and therefore, it’s been selected as the best traditional model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6e31d-65b5-4a40-a923-38125bef73ed",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Objective: Evaluate the performance of the model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bada46-70f7-466e-afc9-b15bfd986efe",
   "metadata": {},
   "source": [
    "### 6.1 Training Both Logistic Regression and DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f40e26-63d8-44e5-a4b7-f9080a826273",
   "metadata": {},
   "source": [
    "#### 6.1.1 Train Logistic Regression on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea7a54c-f849-4754-b651-8bf50e04201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Classification Report (Training Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63      8939\n",
      "           1       0.63      0.65      0.64      8938\n",
      "\n",
      "    accuracy                           0.64     17877\n",
      "   macro avg       0.64      0.64      0.64     17877\n",
      "weighted avg       0.64      0.64      0.64     17877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_logistic_model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "best_logistic_model.fit(X_train_combined, y_train)\n",
    "\n",
    "# Evaluate Logistic Regression on the training set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logistic_predictions = best_logistic_model.predict(X_train_combined)\n",
    "print(\"\\nLogistic Regression Classification Report (Training Set):\")\n",
    "print(classification_report(y_train, logistic_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0c457-da47-40fa-9cee-a07d723e9cf5",
   "metadata": {},
   "source": [
    "#### 6.1.2 Training DistilBERT\n",
    "\n",
    "Fine-tune DistilBERT using the same cleaned text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e8edde-1c3d-472b-a4e3-7236eb92bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'transformers[torch]' accelerate\n",
    "\n",
    "# !pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54401de5-8d49-451e-9974-008c77631071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accelerate\n",
      "Version: 1.3.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6af1f17f-687b-403c-8a0f-3742aaff97d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3354' max='3354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3354/3354 08:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.698700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.685600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.701400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.701500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.693900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.693900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.699300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.698400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.701400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.730600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.698700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.701500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.694000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.695600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.688700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.696200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>0.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>0.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>0.694800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>0.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.704300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>0.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>0.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./distilbert_finetuned/tokenizer_config.json',\n",
       " './distilbert_finetuned/special_tokens_map.json',\n",
       " './distilbert_finetuned/vocab.txt',\n",
       " './distilbert_finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_dataset = TextDataset(\n",
    "    texts=training_data['Final_Cleaned_Text'].tolist(),\n",
    "    labels=training_data['Label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "# Initialize DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\"  # Fixed deprecation warning\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train DistilBERT\n",
    "trainer.train()\n",
    "\n",
    "# Save DistilBERT for later comparison\n",
    "model.save_pretrained('./distilbert_finetuned')\n",
    "tokenizer.save_pretrained('./distilbert_finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92822a-42e6-4df6-8a48-4fa6d1dc3aa5",
   "metadata": {},
   "source": [
    "### Evaluating the DistilBERT Model After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "560b1e65-bf87-435b-8e4d-3ac3f0c4fb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.05      0.10      8939\n",
      "           1       0.51      0.97      0.67      8938\n",
      "\n",
      "    accuracy                           0.51     17877\n",
      "   macro avg       0.58      0.51      0.38     17877\n",
      "weighted avg       0.58      0.51      0.38     17877\n",
      "\n",
      "Accuracy: 0.5132\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained('./distilbert_finetuned')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('./distilbert_finetuned')\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to tokenize and prepare the dataset for inference\n",
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "# Tokenize the training dataset (or test set, if available)\n",
    "encodings = tokenize_texts(training_data['Final_Cleaned_Text'].tolist(), tokenizer)\n",
    "\n",
    "# Move tensors to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# Get predictions from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convert logits to predicted class labels (0 or 1)\n",
    "predicted_labels = torch.argmax(logits, axis=1).cpu().numpy()\n",
    "\n",
    "# Evaluate the predictions\n",
    "true_labels = training_data['Label'].values\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, predicted_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd014a1-4ae1-408a-a0cd-9a251a2c38d4",
   "metadata": {},
   "source": [
    "## 7. Post-Modeling and Saving the Model\n",
    "\n",
    "Objective: Save the trained model and provide a mechanism for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828d031-79a2-4527-984b-cef519cd6592",
   "metadata": {},
   "source": [
    "### 7.1 Saving the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63cd959b-39b2-4b0f-a0ab-692ffcac5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model saved to best_logistic_model.pkl.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained Logistic Regression model\n",
    "logistic_model_path = \"best_logistic_model.pkl\"\n",
    "joblib.dump(best_logistic_model, logistic_model_path)\n",
    "print(f\"Logistic Regression model saved to {logistic_model_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4da5c-0d5a-47c1-80f2-e9941b2bebff",
   "metadata": {},
   "source": [
    "### 7.2 Saving the DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "457af612-eab1-4feb-9b71-617b9f4720e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT model and tokenizer saved to ./distilbert_finetuned.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Save the DistilBERT model and tokenizer\n",
    "distilbert_model_path = \"./distilbert_finetuned\"\n",
    "model.save_pretrained(distilbert_model_path)\n",
    "tokenizer.save_pretrained(distilbert_model_path)\n",
    "print(f\"DistilBERT model and tokenizer saved to {distilbert_model_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da93e3-2520-472a-9fef-51d2d8aac283",
   "metadata": {},
   "source": [
    "## 8. Testing on New Data\n",
    "\n",
    "Objective: Test the saved model on new datasets and classify the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b2ba7b-e25c-400f-adc1-14379a716e8a",
   "metadata": {},
   "source": [
    "### 8.1 Inference Function for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2469f11-b78c-4873-a07f-21d16d1f82dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression predictions saved to logistic_regression_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Recompute additional features for the real dataset\n",
    "def extract_additional_features(texts):\n",
    "    char_lengths = np.array([len(text) for text in texts]).reshape(-1, 1)\n",
    "    word_lengths = np.array([len(text.split()) for text in texts]).reshape(-1, 1)\n",
    "    avg_word_lengths = np.array([np.mean([len(word) for word in text.split()]) if len(text.split()) > 0 else 0\n",
    "                                 for text in texts]).reshape(-1, 1)\n",
    "    punctuation_counts = np.array([sum(1 for char in text if char in \".,!?;:\") / len(text)\n",
    "                                   if len(text) > 0 else 0 for text in texts]).reshape(-1, 1)\n",
    "    return np.hstack([char_lengths, word_lengths, avg_word_lengths, punctuation_counts])\n",
    "\n",
    "# Extract and combine TF-IDF + additional features\n",
    "additional_features_real = extract_additional_features(real_data['Final_Cleaned_Text'])\n",
    "tfidf_features_real = tfidf_vectorizer.transform(real_data['Final_Cleaned_Text'])\n",
    "combined_features_real = hstack([tfidf_features_real, additional_features_real])\n",
    "\n",
    "# Predict using the saved Logistic Regression model\n",
    "logistic_predictions = best_logistic_model.predict(combined_features_real)\n",
    "\n",
    "# Store predictions\n",
    "real_data_logistic = real_data.copy()\n",
    "real_data_logistic['Prediction'] = logistic_predictions\n",
    "real_data_logistic['Prediction_Label'] = real_data_logistic['Prediction'].apply(lambda x: 'Human' if x == 1 else 'Machine')\n",
    "\n",
    "# Save predictions to CSV\n",
    "logistic_output_path = \"logistic_regression_predictions.csv\"\n",
    "real_data_logistic[['Text', 'Prediction_Label']].to_csv(logistic_output_path, index=False)\n",
    "print(f\"Logistic Regression predictions saved to {logistic_output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3e9a4-fc3b-4782-8fc9-97ffc84195b4",
   "metadata": {},
   "source": [
    "### 8.2 Inference Function for DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82aab001-0a49-455b-aad9-abfd95281e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Yo no creo que a nadie le haya encantado un pene flácido .\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: No va a resolver sus problemas de crédito o mejorar su relación con su padre .\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: Te encantará este !\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: Yo estaba a volar a un aeropuerto varias horas de distancia , alquilar un coche , conducir a un lugar remoto en Canadá , y desactivar el teléfono .\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: ( Maid En Manhattan , The Wedding Planner , Jersey Girl , Monster In Law , , Gigli , The Back-Up Plan , ¿ Qué esperar cuando se está esperando )\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: Mi padre llegó con la primera ola de fuerzas aliadas en el día D , 6 de junio 1944 .\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: Y podemos todos estar de acuerdo que los envases no miente ?\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: Por supuesto , todos los compañeros de reparto de Casey en Happy Endings estaban allí también , como Elisha Cuthbert y Damon Wayans Jr .\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: Al estilo chino capitalismo autoritario - sí mismo un producto de las lecciones aprendidas por los líderes leninistas de China desde la caída del Muro - parece más atractivo para mucha gente fuera al oeste tradicional , mientras que el capitalismo desenfrenado , desigual occidental financiera ( también en parte a Hubris post-1989 ) miradas menos .\n",
      "Prediction: Human\n",
      "\n",
      "Sentence: Si sólo abrasador culpa y la envidia fuera de lugar no eran tan maldito tiempo .\n",
      "Prediction: Human\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def infer_distilbert(model_path, tokenizer_path, new_sentences):\n",
    "    \"\"\"\n",
    "    Loads the saved DistilBERT model and performs inference on new sentences.\n",
    "    \"\"\"\n",
    "    # Load the saved model and tokenizer\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Tokenize the new sentences\n",
    "    encodings = tokenizer(\n",
    "        new_sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, axis=1).cpu().numpy()\n",
    "\n",
    "    # Return predictions (0 for machine, 1 for human)\n",
    "    return predictions\n",
    "\n",
    "# Example: Classify sentences from the real dataset\n",
    "distilbert_predictions = infer_distilbert(\"./distilbert_finetuned\", \"./distilbert_finetuned\", real_sentences)\n",
    "\n",
    "# Display predictions\n",
    "for i, sentence in enumerate(real_data['Text'].head(10)):\n",
    "    print(f\"Sentence: {sentence}\\nPrediction: {'Human' if distilbert_predictions[i] == 1 else 'Machine'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde8db8-9041-4b5f-8d79-2abfd842eea2",
   "metadata": {},
   "source": [
    "### 8.3 Save Predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00b2d51d-34aa-4d15-a321-b0919a507075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to classified_real_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save predictions from the chosen model (DistilBERT example shown)\n",
    "real_data['Prediction'] = distilbert_predictions\n",
    "real_data['Prediction_Label'] = real_data['Prediction'].apply(lambda x: 'Human' if x == 1 else 'Machine')\n",
    "\n",
    "# Save to CSV\n",
    "real_data[['Text', 'Prediction_Label']].to_csv('classified_real_data.csv', index=False)\n",
    "print(\"Predictions saved to classified_real_data.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
