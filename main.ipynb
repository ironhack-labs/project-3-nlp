{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb33a4f-4490-4a6e-9f4b-84c4cdefc6bf",
   "metadata": {},
   "source": [
    "# Natural Language Processing Challenge\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Learning how to process text is a skill required for Data Scientists. In this project, you will put these skills into practice to identify whether a sentence was automatically translated or translated by a human.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this repository you will find dataset containing sentences in Spanish and their tags: 0, if the sentences was translated by a Machine, 1, if the sentence was translated by a professional translator. Your goal is to build a classifier that is able to distinguish between the two.\n",
    "\n",
    "## Guidance\n",
    "Like in a real life scenario, you are able to make your own choices and text treatment. Use the techniques you have learned and the common packages to process this data and classify the text.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "1. **Python Code:** Provide well-documented Python code that conducts the analysis.\n",
    "2. **Accuracy estimation:** Provide the teacher with your estimation of how your model will perform.\n",
    "2. **Classified Dataset**: On Friday, you will receive a dataset without tags. Prepare your code to be able to tag that dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48754302-7ff3-4ef8-937e-ab40b941c491",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "### 1. Preprocessing: Tokenization, lowercasing, stopword removal, and lemmatization.\n",
    "### 2. Feature Extraction:\n",
    "    - Traditional: TF-IDF or n-gram frequency analysis.\n",
    "    - Deep Learning: Word embeddings (FastText, Word2Vec, or pretrained ones like BERT).\n",
    "### 3. Model Selection:\n",
    "    - Traditional ML: Logistic Regression, Random Forest, or SVM (good for TF-IDF).\n",
    "    - Deep Learning: LSTM, CNN, or a Transformer-based model like DistilBERT.\n",
    "### 4. Evaluation: Train/test split, cross-validation, and metrics like accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efd36a-c3f4-4a66-84f6-62271e1aa000",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7692c42f-f672-4849-a1cb-1aece153c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e1f9cea-bd5e-4a59-a926-2ff00b2130fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                           sentence\n",
      "0      1  Cuando conocí a Janice en 2013 , una familia n...\n",
      "1      0  Hwang habló en Sur de este año por Southwest M...\n",
      "2      1  Usted podría pensar Katy Perry y Robert Pattin...\n",
      "3      1  Cualquiera que haya volado los cielos del crea...\n",
      "4      1  Bueno , este cantante tendrá un LARGO tiempo p...\n"
     ]
    }
   ],
   "source": [
    "# Load the data (adjust the path as necessary)\n",
    "file_path = 'TRAINING_DATA.TXT'\n",
    "data = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'sentence'])\n",
    "\n",
    "# Check the first few rows to ensure it loaded correctly\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f7321d-93d2-460b-a480-e9878e3910da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                sentence  predicted_label  \\\n",
      "0      Cuando conocí a Janice en 2013 , una familia n...                0   \n",
      "1      Hwang habló en Sur de este año por Southwest M...                0   \n",
      "2      Usted podría pensar Katy Perry y Robert Pattin...                0   \n",
      "3      Cualquiera que haya volado los cielos del crea...                0   \n",
      "4      Bueno , este cantante tendrá un LARGO tiempo p...                0   \n",
      "...                                                  ...              ...   \n",
      "14919   Yo voy a hacer canciones y voy a hablar de la...                0   \n",
      "14920  Si la gente en ESA posición todavía se sintier...                0   \n",
      "14921  ) De todos modos , después de eso , ella es ca...                0   \n",
      "14922                    ( \" No hay humo en el pasillo .                0   \n",
      "14923   Ella repetía , como hemos luchado durante ves...                0   \n",
      "\n",
      "                                      confidence  \n",
      "0       [0.5526164770126343, 0.4473835527896881]  \n",
      "1      [0.6167416572570801, 0.38325831294059753]  \n",
      "2      [0.5445879697799683, 0.45541203022003174]  \n",
      "3         [0.589755117893219, 0.410244882106781]  \n",
      "4         [0.623134970664978, 0.376865029335022]  \n",
      "...                                          ...  \n",
      "14919   [0.5484759211540222, 0.4515240788459778]  \n",
      "14920  [0.5269243717193604, 0.47307562828063965]  \n",
      "14921   [0.5291897058486938, 0.4708103537559509]  \n",
      "14922   [0.5819475054740906, 0.4180524945259094]  \n",
      "14923  [0.6055747270584106, 0.39442530274391174]  \n",
      "\n",
      "[14924 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained Spanish BERT Model (No Training)\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Step 1: Clean Text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-záéíóúñ\\s]', '', text)  # Remove special characters and punctuation\n",
    "    return text\n",
    "\n",
    "# Function to Predict Label for a Given Sentence\n",
    "def predict_label(sentence):\n",
    "    cleaned_sentence = clean_text(sentence)\n",
    "    inputs = tokenizer(cleaned_sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation (faster inference)\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return predicted_class, probabilities.tolist()[0]  # Return label and confidence scores\n",
    "\n",
    "# Step 2: Process the Entire Dataset\n",
    "def process_dataset(dataset):\n",
    "    predictions = []\n",
    "    for sentence in dataset:\n",
    "        label, confidence = predict_label(sentence)\n",
    "        predictions.append({\n",
    "            \"sentence\": sentence,\n",
    "            \"predicted_label\": label,  # 0=Machine, 1=Professional\n",
    "            \"confidence\": confidence\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "# Step 3: Process and Get Predictions\n",
    "predicted_df = process_dataset(dataset['sentence'])\n",
    "\n",
    "# Print predictions for all sentences\n",
    "print(predicted_df)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "predicted_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe07f0b9-4d76-4aac-93c7-3fd7dee2e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5015\n",
      "Precision: 0.5374\n",
      "Recall: 0.0202\n",
      "F1 Score: 0.0390\n",
      "Confusion Matrix:\n",
      "[[7334  130]\n",
      " [7309  151]]\n"
     ]
    }
   ],
   "source": [
    "true_labels = data['label'].tolist()\n",
    "\n",
    "# Get predicted labels from the model\n",
    "predicted_labels = predicted_df['predicted_label'].tolist()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e9ef3-1082-41e1-8d6c-32e6556513a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
